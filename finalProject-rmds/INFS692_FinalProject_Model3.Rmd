---
title: "INFS 692 Data Science Final Project Model 3"
author: "Ivan Huang"
date: '2022-12-09'
output:
  html_document:
    df_print: paged
---

# Model 3

**Without considering the _binary output_ and _categorical variables_** in the data set, compare the following clustering technique results:

## Step 1: inport the data set and essential R libraries
```{r}
# import libraries
##################
# Helper packages
library(dplyr)       # for data manipulation
library(ggplot2)     # for data visualization
library(stringr)     # for string functionality
library(gridExtra)   # for manipulaiting the grid

# Modeling packages
library(tidyverse)  # data manipulation
library(cluster)     # for general clustering algorithms
library(factoextra)  # for visualizing cluster results

# Modeling packages
library(mclust)   # for fitting clustering algorithms

# load essential data frame
df_m3 <-  read.csv('./radiomics_completedata.csv')
```
## Step 2: Conduct K-Means clustering

```{r}
# remove categorical and binary columns from the data frame
i1_m3 <- sapply(df_m3, is.numeric)

df_m3 <- df_m3[i1_m3]

df_m3 <- Filter(function(x) !all(x %in% c(0, 1)), df_m3)

# Check if there's any null values
df_m3 <- na.omit(df_m3)

# scale the data 
final_m3 <- scale(df_m3)

final_m3 <- as.data.frame(final_m3)
```

Start the Clustering process for K-Means
```{r}
# Determining Optimal Number of Clusters
set.seed(123)

#function to compute total within-cluster sum of square 
wss <- function(k) {
  kmeans(final_m3, k, nstart = 10)$tot.withinss
}

# Compute and plot wss for k = 1 to k = 15
k.values <- 1:15

# extract wss for 2-15 clusters
wss_values <- map_dbl(k.values, wss)

plot(k.values, wss_values,
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")

#or use this
fviz_nbclust(final_m3, kmeans, method = "silhouette")

# compute gap statistic
set.seed(123)
gap_stat <- clusGap(final_m3, FUN = kmeans, nstart = 25,
                    K.max = 10, B = 50)
# Print the result
print(gap_stat, method = "firstmax")

fviz_gap_stat(gap_stat)

# Compute k-means clustering with k = 2
set.seed(123)
final <- kmeans(final_m3, 2, nstart = 25)
print(final)

#final data
fviz_cluster(final, data = final_m3)
```


## Step 3: Hierarchical

Now that we have done the K-Means clustering, we are aiming for unsupervised learning using hierarchical model

```{r}
# Dissimilarity matrix
d <- dist(final_m3, method = "euclidean")

# Plot cluster results
p1 <- fviz_nbclust(final_m3, FUN = hcut, method = "wss", 
                   k.max = 10) +
  ggtitle("(A) Elbow method")
p2 <- fviz_nbclust(final_m3, FUN = hcut, method = "silhouette", 
                   k.max = 10) +
  ggtitle("(B) Silhouette method")
p3 <- fviz_nbclust(final_m3, FUN = hcut, method = "gap_stat", 
                   k.max = 10) +
  ggtitle("(C) Gap statistic")

# Display plots side by side
gridExtra::grid.arrange(p1, p2, p3, nrow = 1)

# Construct dendorgram for the given data
hc5 <- hclust(d, method = "ward.D2" )
dend_plot <- fviz_dend(hc5)
dend_data <- attr(dend_plot, "dendrogram")
dend_cuts <- cut(dend_data, h = 2)
dend_cuts
#fviz_dend(dend_cuts$lower[[3]])

# Ward's method
hc5 <- hclust(d, method = "ward.D2" )

# Cut tree into 4 groups
sub_grp <- cutree(hc5, k = 2)

# Number of members in each cluster
table(sub_grp)

# Plot full dendogram
fviz_dend(
  hc5,
  k = 2,
  horiz = TRUE,
  rect = TRUE,
  rect_fill = TRUE,
  rect_border = "jco",
  k_colors = "jco",
  cex = 0.1
)

dend_plot <- fviz_dend(hc5)                # create full dendogram
dend_data <- attr(dend_plot, "dendrogram") # extract plot info
dend_cuts <- cut(dend_data, h = 70.5)      # cut the dendogram at 
# designated height

# Create sub dendrogram plots
p1 <- fviz_dend(dend_cuts$lower[[1]])
p2 <- fviz_dend(dend_cuts$lower[[1]], type = 'circular')

# Side by side plots
gridExtra::grid.arrange(p1, p2, nrow = 1)
```

## Step4: model-based

There is one problem for the model-based clustering techniques. In the model based clustering techniques since we have too much data, the plot would not be able to generate correctly. An Error for plot.new() indicating `figure margin is too large` will always occur even if we set the margin to `c(1,1,1,1)`.

Therefore, to apply GMM model with 3 components and plot the results correctly, I only chose four columns from the original data frame: `Failure`, `Entropy_cooc.W.ADC`, `Entropy_hist.PET` and `Entropy_cooc.L.PET`.



```{r}
# Apply GMM model with 3 components
df_failure <- select(final_m3, Failure, Entropy_cooc.W.ADC, Entropy_hist.PET, Entropy_cooc.L.PET)
arrest_mc <- Mclust(df_failure, G = 3)

# Plot results
par(mar=c(1,1,1,1))
plot(arrest_mc, what = "density")
plot(arrest_mc, what = "uncertainty")

# Observations with high uncertainty
sort(arrest_mc$uncertainty, decreasing = TRUE) %>% head()


summary(arrest_mc)

arrest_optimal_mc <- Mclust(df_failure)

summary(arrest_optimal_mc)

legend_args <- list(x = "bottomright", ncol = 5)
plot(arrest_optimal_mc, what = 'BIC', legendArgs = legend_args)
plot(arrest_optimal_mc, what = 'classification')
plot(arrest_optimal_mc, what = 'uncertainty')

df_mc <- Mclust(df_failure, 1:20)

summary(df_mc)

plot(df_mc, what = 'BIC', 
     legendArgs = list(x = "bottomright", ncol = 5))

probabilities <- df_mc$z 

probabilities <- probabilities %>%
  as.data.frame() %>%
  mutate(id = row_number()) %>%
  tidyr::gather(cluster, probability, -id)

ggplot(probabilities, aes(probability)) +
  geom_histogram() +
  facet_wrap(~ cluster, nrow = 2)

uncertainty <- data.frame(
  id = 1:nrow(df_failure),
  cluster = df_mc$classification,
  uncertainty = df_mc$uncertainty
)

uncertainty %>%
  group_by(cluster) %>%
  filter(uncertainty > 0.0001) %>%
  ggplot(aes(uncertainty, reorder(id, uncertainty))) +
  geom_point() +
  facet_wrap(~ cluster, scales = 'free_y', nrow = 1)


cluster2 <- df_failure %>%
  scale() %>%
  as.data.frame() %>%
  mutate(cluster = df_mc$classification) %>%
  filter(cluster == 2) %>%
  select(-cluster)

cluster2 %>%
  tidyr::gather(product, std_count) %>%
  group_by(product) %>%
  summarize(avg = mean(std_count)) %>%
  ggplot(aes(avg, reorder(product, avg))) +
  geom_point() +
  labs(x = "Average standardized consumption", y = NULL)
```


**However**, if I do the model-based clustering for the entire data set, it would not be able to plot the graphs since every time it will return the `figure margin is too large` error. Therefore, without plotting the `density`, `uncetainty`, and `classification` graphs, one should be able to reach the results as followed:

```{r}
# Apply GMM model with 3 components
arrest_final_mc <- Mclust(final_m3, G = 3)

# Plot results
# par(mar=c(1,1,1,1))
# plot(arrest_mc, what = "density")
# plot(arrest_mc, what = "uncertainty")

# Observations with high uncertainty
sort(arrest_final_mc$uncertainty, decreasing = TRUE) %>% head()

summary(arrest_mc)

arrest_optimal_final <- Mclust(final_m3)

summary(arrest_optimal_final)

legend_args <- list(x = "bottomright", ncol = 5)
#plot(arrest_optimal_final, what = 'BIC', legendArgs = legend_args)
#plot(arrest_optimal_final, what = 'classification')
#plot(arrest_optimal_final, what = 'uncertainty')

df_finalmc <- Mclust(final_m3, 1:20)

summary(df_finalmc)

# plot(df_finalmc, what = 'BIC', 
#     legendArgs = list(x = "bottomright", ncol = 5))

probabilities <- df_finalmc$z 

probabilities <- probabilities %>%
  as.data.frame() %>%
  mutate(id = row_number()) %>%
  tidyr::gather(cluster, probability, -id)

ggplot(probabilities, aes(probability)) +
  geom_histogram() +
  facet_wrap(~ cluster, nrow = 2)

uncertainty <- data.frame(
  id = 1:nrow(final_m3),
  cluster = df_finalmc$classification,
  uncertainty = df_finalmc$uncertainty
)

uncertainty %>%
  group_by(cluster) %>%
  filter(uncertainty > 0.0001) %>%
  ggplot(aes(uncertainty, reorder(id, uncertainty))) +
  geom_point() +
  facet_wrap(~ cluster, scales = 'free_y', nrow = 1)


cluster2 <- final_m3 %>%
  scale() %>%
  as.data.frame() %>%
  mutate(cluster = df_finalmc$classification) %>%
  filter(cluster == 2) %>%
  select(-cluster)

cluster2 %>%
  tidyr::gather(product, std_count) %>%
  group_by(product) %>%
  summarize(avg = mean(std_count)) %>%
  ggplot(aes(avg, reorder(product, avg))) +
  geom_point() +
  labs(x = "Average standardized consumption", y = NULL)
```

